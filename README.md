# Retrieval-Augmented Generation (RAG) System with Python & Ollama ğŸš€  

## Overview  
This repository contains a **Jupyter Notebook** that demonstrates how to build a **Retrieval-Augmented Generation (RAG) system** using:  
- **Python** ğŸ  
- **Ollama** (for running LLM locally)  
- **LangChain** (for retrieval & orchestration)  
- **ChromaDB** (for vector storage)  
- **SentenceTransformers** (for text embeddings)  

### ğŸ”¹ What You'll Learn  
âœ… What is RAG, and how does it work?  
âœ… How to extract text from PDFs  
âœ… How to create embeddings and store them in ChromaDB  
âœ… How to retrieve relevant information  
âœ… How to use Ollamaâ€™s LLM to generate responses  

---

## ğŸ“Œ Installation  

1ï¸âƒ£ **Clone this repository:**  
```bash
git clone https://github.com/yourusername/rag-system.git
cd rag-system
```

3ï¸âƒ£ **Create a virtual environment & install dependencies:**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install langchain chromadb sentence-transformers pypdf ollama
```
4ï¸âƒ£ **Pull LLM Model:**
```bash
ollama pull llama3.2
```

## ğŸ›  Usage
### **Run the Jupyter Notebook (rag_system.ipynb) step-by-step to:**


âœ… Load and process PDF documents  
âœ… Generate text embeddings using SentenceTransformers  
âœ… Store embeddings in ChromaDB for efficient retrieval  
âœ… Query and retrieve relevant documents  
âœ… Use Ollamaâ€™s LLM to generate responses  

